假设有一元函数$f(x)=x^2$

自变量只有两个方向，朝右运动，函数值增加，称为梯度方向$\nabla f(x)$；朝左运动，函数值减少，称为梯度方向的反方向$-\nabla f(x)$。

梯度：                      $\nabla f(x)=2x$
初始值：                   $x_0=10$
学习率（步长）：    $\eta=0.2$
终止条件：               $\| \nabla f(x) \| <= 0.01$

推导：
$x_1=x_0-\eta~\nabla f(x_0)$      $x_1=6$            $\| \nabla f(x_0) \| = 12$
$x_2=x_1-\eta~\nabla f(x_1)$      $x_2=3.6$         $\| \nabla f(x_1) \| = 7.2$
$x_3=x_2-\eta~\nabla f(x_2)$      $x_3=2.16$       $\| \nabla f(x_2) \| = 4.32$
$x_4=x_3-\eta~\nabla f(x_3)$      $x_4=1.3$         $\| \nabla f(x_3) \| = 2.59$
...
$x_{15}=x_{14}-\eta~\nabla f(x_{14})$      $x_{15}=0.005$         $\| \nabla f(x_{15}) \| = 0.01$


问题一：减去$\eta~\nabla f(x)$ 是什么意思？
从微分的角度看，$\nabla f(x)$ 是导数，表示局部微小的变化率，这个变化率乘以学习率（步长），就表示沿着切线方向前进的距离。


梯度下降法有一个最核心的点，我觉得大家都没有关注到，而且似乎不论在哪我都看不到有人讨论这个。  
我想新人看到在凹的二次函数图像上应用梯度下降时心里肯定有疑问。我已经能够根据函数图像或者已经探索的点的位置能够模糊地找到误差函数最低点的位置，甚至可以使用公式求点，为什么还要用梯度下降法迭代许多遍？  
事实上，根据神经网络模型所表示出来的函数，与真实误差函数的函数图像可能相去甚远(这才是关键点)。我以前做过实验，如果认为模型所表示的函数即为真实误差函数一样，那么模型最终将无法被优化。如果每次优化，采用二次探测法(即在当前位置分别模拟对学习率翻2的i次方倍进行梯度下降，选取优化后误差函数值最小的学习率进行实际优化。懒得在最优区间里寻找最优点了，也可以选择二次探测法，同样时间复杂度可以找到最优点)，将模型优化到一个较为接近当前误差函数值最低点的点，那么最终结果就就是学习率常常要翻几千几万倍，误差值越来越大。  
举下山的例子来说人话。每向山下走一小步，你就会发现身边山形又有大变动，原先最低点的位置现在又不知道跑到哪个斜坡上去了。如果你每次都大跨步，甚至一下就跨到山谷(实际上这是可行的，只不过那个谷底是假谷底)，那么优化完一次后山形发生变化，你又到高处去了。最终结果就是你根本到不了山谷。如果每次只走一小步，尽管你看到的山形不断变化，但幅度并不是很剧烈。长期来看，误差函数给予的启发式信息还是足够你走到谷底的。  
为什么走一步后山形会变化？因为模型根据梯度下降法优化后，模型参数变化，误差函数也跟着变。而所谓山形，就是误差函数的图形。模型所表达出的误差函数并非真实误差函数，真实误差函数是不容易知道的。每走一步你看到的山形都将变化，而你可能始终都无法看到这座山本来的面貌。  
这些实验是很久前做的，细节不太记得了，如果表达有不准确的地方还望指正。